{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Simple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, it can be written as \n",
    "\n",
    "\n",
    "\\begin{equation*} \n",
    "Y \\approx \\beta_0 + \\beta_1 X \n",
    "\\end{equation*}\n",
    "where $\\beta_0$ $\\beta_1 are two unknown constant or parameter / coefficients of the model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Estimating the coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i$  . Hence $e_i = y_i -\\hat{y_i} $ \n",
    "\n",
    "We define residual sum of square (RSS) as :  $RSS = e_1^2 + e_2^2 + ...+ e_n^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Assessing the accuracy of the coefficient estimates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true relation ship is generally not known for real data , but the least squares line can always be computed using the coefficient estimates \n",
    "- The property of unbiasedness holds for the least squares coefficient estimates as well. If we estimate $\\beta_1 $ and $\\beta_0 $ on the basis of a particular data set, then our esimtates wont be exactly equal to $\\beta_0$ and $\\beta_1$\n",
    "- Residual Standard Error (RSE ) = $\\sqrt{\\frac{RSS}{n-2}}$\n",
    "- Standard error is also used to perform Hypothesis testing \n",
    "    - $H_0$ : there is no relationship between X and Y or $\\beta_1 = 0$\n",
    "    - $H_1$ : There is some relationship between X and Y $\\beta_1 \\neq 0$\n",
    "    - t-statistic is computed as follow $t = \\frac{\\hat{\\beta_1} -0}{SE(\\hat{\\beta_1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Assessing the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RSE (RSE ) = $\\sqrt{\\frac{RSS}{n-2}}$\n",
    "- $R^2$ statistic \n",
    "    - $R^2 = 1- \\frac{RSS}{TSS}$  where $TSS = \\sum(y_i - \\bar{y})^2$\n",
    "    - $R^2$ measures the proportion of variability in Y that can be explained using X \n",
    "    - A number near 0 indicates taht the regression did not explain much of the variability in the response , this might occur because the linear model is wrong , or the inherent error $\\sigma^2 $ is high\n",
    "    - Correcation is defined as $ Cor(X,Y) = \\frac{\\sum_{i=1}^n(x_i -\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i -\\bar{x}})\\sqrt{\\sum_{i=1}^n(y_i -\\bar{y})}} $\n",
    "    - For simple linear regression, they are the same\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can make predictions in the form of: $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 +..+ \\hat{\\beta_p}x_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Some important questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random stuff\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
