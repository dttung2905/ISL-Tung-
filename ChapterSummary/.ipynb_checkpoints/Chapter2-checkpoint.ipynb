{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Simple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, it can be written as \n",
    "\n",
    "\n",
    "\\begin{equation*} \n",
    "Y \\approx \\beta_0 + \\beta_1 X \n",
    "\\end{equation*}\n",
    "where $\\beta_0$ $\\beta_1 are two unknown constant or parameter / coefficients of the model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Estimating the coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i$  . Hence $e_i = y_i -\\hat{y_i} $ \n",
    "\n",
    "We define residual sum of square (RSS) as :  $RSS = e_1^2 + e_2^2 + ...+ e_n^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Assessing the accuracy of the coefficient estimates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true relation ship is generally not known for real data , but the least squares line can always be computed using the coefficient estimates \n",
    "- The property of unbiasedness holds for the least squares coefficient estimates as well. If we estimate $\\beta_1 $ and $\\beta_0 $ on the basis of a particular data set, then our esimtates wont be exactly equal to $\\beta_0$ and $\\beta_1$\n",
    "- Residual Standard Error (RSE ) = $\\sqrt{\\frac{RSS}{n-2}}$\n",
    "- Standard error is also used to perform Hypothesis testing \n",
    "    - $H_0$ : there is no relationship between X and Y or $\\beta_1 = 0$\n",
    "    - $H_1$ : There is some relationship between X and Y $\\beta_1 \\neq 0$\n",
    "    - t-statistic is computed as follow $t = \\frac{\\hat{\\beta_1} -0}{SE(\\hat{\\beta_1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Assessing the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RSE (RSE ) = $\\sqrt{\\frac{RSS}{n-2}}$\n",
    "- $R^2$ statistic \n",
    "    - $R^2 = 1- \\frac{RSS}{TSS}$  where $TSS = \\sum(y_i - \\bar{y})^2$\n",
    "    - $R^2$ measures the proportion of variability in Y that can be explained using X \n",
    "    - A number near 0 indicates taht the regression did not explain much of the variability in the response , this might occur because the linear model is wrong , or the inherent error $\\sigma^2 $ is high\n",
    "    - Correcation is defined as $ Cor(X,Y) = \\frac{\\sum_{i=1}^n(x_i -\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i -\\bar{x}})\\sqrt{\\sum_{i=1}^n(y_i -\\bar{y})}} $\n",
    "    - For simple linear regression, they are the same\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can make predictions in the form of: $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 +..+ \\hat{\\beta_p}x_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Some important questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Is there really a relationship between the Response and Predictor\n",
    "We use hypothesis testing to answer this question\n",
    "- $H_0 = \\beta_1 = \\beta_2 = \\beta_3 = .. = \\beta_p = 0$\n",
    "- $H_1$ at leas one $\\beta_j$ is non zero\n",
    "\n",
    "Computing the F-statistic:\n",
    "- $F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$\n",
    "\n",
    "If the $H_1$ is true, the F-statistic is going to be greater than 1 and otherwise.\n",
    "\n",
    "But the question is what if the F-statistic is closer to 1, how do we determine whether to reject or not? \n",
    "\n",
    "The answer is when n is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$\n",
    "\n",
    "We need to look at individual F-statistic for each features as well as the overall F-statistic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Which are the important factors ?\n",
    "\n",
    "we can use either one : Adjusted R-square / Bayesian information criterion (BIC) \n",
    "\n",
    "THere are also classical approach for this task:\n",
    "- Forward selection: start with no feature, and gradually add in feature that result in the lowest RSS\n",
    "- Backward selection: start with all the features and gradually eliminate those with the largest value until a stopping rule is reached \n",
    "- Mix selection: This is a combination of forward and backward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Model Fit ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding more feature always result in an increase in the R-square value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
