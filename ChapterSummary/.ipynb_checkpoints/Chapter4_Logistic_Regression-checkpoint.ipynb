{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. An overview of Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Why not Linear Regression?\n",
    "\n",
    "- We dont use it when there are multiple outcomes, encoding each outcome with a numeric number will imply an ordering of the outcome\n",
    "- for a binary response with 0/1 coding as above, regression by least squares does make sense. However, if we use linear  regression , some of our estimates might be outside the [0,1] interval , making them hard to intepret as probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal : Logistic Regression models the probability that Y belongs to a particular category\n",
    "- We use the logistic function for this \n",
    "\\begin{equation*} \n",
    " p(X) = \\frac{e^{\\beta_0+\\beta_1 X} }{1 + e^{\\beta_0+\\beta_1 X} }\n",
    "\\end{equation*}\n",
    "- The probability can be small , but never be zero. And on the other side, it can be very large, but never exceeds 1\n",
    "- It will always produce S-shaped curve , regardless of the value of X \n",
    "- With a bit of manipulation , we have \n",
    "\\begin{equation*} \n",
    "odds = \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1 X} \n",
    "\\end{equation*}\n",
    "\n",
    "- Then log-odds\n",
    "\\begin{equation*} \n",
    "log-odds = log(\\frac{p(X)}{1-p(X)}) = \\beta_0+\\beta_1 X\n",
    "\\end{equation*}\n",
    "\n",
    "- Interpretation: Increasing X by one unite changes the log odds by $\\beta_1$ . However, because the relationship between p(X) and X in the equation above is not a straight line. $\\beta_1$ does not correspond to the change in p(X) associated with a one-unit increase in X . The amount that p(X) changes due to a one-unit change in X will depend on the current value of X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimating the Regression Coefficients\n",
    "- The basic intuition behind using maxium likelihood to fit a logisitc regression model is as follows: we seek estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability of default for each individual using the equation above corresponds as closely as possible to the individual's observed default status \n",
    "- It is called likelihood function:\n",
    "\n",
    "\\begin{equation*} \n",
    "l(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\prod_{i': y_i\\prime = 0} (1-p(x_i\\prime))\n",
    "\\end{equation*}\n",
    "\n",
    "- Example: if $\\hat{\\beta_1} =0.0055$ this indicates that an increase in balance is associated with an increase in the probability of default. To be precise, a one-unit increase in balance is assocaited with an increase in the log odds of default by 0.0055 untis \n",
    "- For binary features( yes/no) we can encode it using 0, 1 and put it inside the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multiple Logistic Regression\n",
    "It can be generalized as followed \n",
    "\\begin{equation*} \n",
    "log-odds = log(\\frac{p(X)}{1-p(X)}) = \\beta_0+\\beta_1 X_1 = ...+ \\beta_p X_p\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic Regression for >2 Response Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we have Discriminant analysis for multiple class classfication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Linear Discriminant Analysis\n",
    "### 2.4.1 Why need it ?\n",
    "\n",
    "- When classes are well-separated, the parameter estimates for the log reg are suprisingly unstable --> LDA doesnt have this problem \n",
    "- If n i small and the dsitribution ofpredictors X is approximately normal in each classes --> LDA perform better \n",
    "- When we have more than 2 classes \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Using Bayes' Theorem\n",
    "- Let $\\pi_k$ represent the overall / prior probability that a randomly chose observation comes from the kth class\n",
    "- Let $f_k(X) = Pr(X=x | Y=k)$ denote the density function of X for an observation that comes from the kth class . $f_k(x)$ is relatively large if there is a high probability that an observation in the kth class has X $\\approx$ x\n",
    "\n",
    "- Bayes' theorem state that\n",
    "\\begin{equation*} \n",
    "Pr(Y=k | X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}{K} \\pi_l f_l(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 ROC Curve\n",
    "- ROC = Receiver operating characteristics \n",
    "- Ideal ROC will hug the top left corner \n",
    "- ROC curve is used to compare different classifier \n",
    "- Varying the classifier threshold changes its true positive and false positive rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
