{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. An overview of Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Why not Linear Regression?\n",
    "\n",
    "- We dont use it when there are multiple outcomes, encoding each outcome with a numeric number will imply an ordering of the outcome\n",
    "- for a binary response with 0/1 coding as above, regression by least squares does make sense. However, if we use linear  regression , some of our estimates might be outside the [0,1] interval , making them hard to intepret as probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal : Logistic Regression models the probability that Y belongs to a particular category\n",
    "- We use the logistic function for this \n",
    "\\begin{equation*} \n",
    " p(X) = \\frac{e^{\\beta_0+\\beta_1 X} }{1 + e^{\\beta_0+\\beta_1 X} }\n",
    "\\end{equation*}\n",
    "- The probability can be small , but never be zero. And on the other side, it can be very large, but never exceeds 1\n",
    "- It will always produce S-shaped curve , regardless of the value of X \n",
    "- With a bit of manipulation , we have \n",
    "\\begin{equation*} \n",
    "odds = \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1 X} \n",
    "\\end{equation*}\n",
    "\n",
    "- Then log-odds\n",
    "\\begin{equation*} \n",
    "log-odds = log(\\frac{p(X)}{1-p(X)}) = \\beta_0+\\beta_1 X\n",
    "\\end{equation*}\n",
    "\n",
    "- Interpretation: Increasing X by one unite changes the log odds by $\\beta_1$ . However, because the relationship between p(X) and X in the equation above is not a straight line. $\\beta_1$ does not correspond to the change in p(X) associated with a one-unit increase in X . The amount that p(X) changes due to a one-unit change in X will depend on the current value of X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimating the Regression Coefficients\n",
    "- The basic intuition behind using maxium likelihood to fit a logisitc regression model is as follows: we seek estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability of default for each individual using the equation above corresponds as closely as possible to the individual's observed default status \n",
    "- It is called likelihood function:\n",
    "\n",
    "\\begin{equation*} \n",
    "l(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1} p(x_i) \\prod_{i': y_i\\prime = 0} (1-p(x_i\\prime))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
